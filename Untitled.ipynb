{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0eaa11-8787-4503-8178-35b6c710b487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "563360c9-0d89-43ee-873b-470f2518a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from config import *\n",
    "from torchvision.transforms import Normalize, ToTensor\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 directory,\n",
    "                 transform_mode='train'):\n",
    "        self.Normalize = Normalize(mean=[.485, .456, .406],\n",
    "                                   std=[.229, .224, .225])\n",
    "        self.ToTensor = ToTensor()\n",
    "        self.images = sorted(glob(directory + '/image/*.jpg'))\n",
    "        self.masks = sorted(glob(directory + '/mask2/*.png'))\n",
    "\n",
    "        assert len(self.images) == len(self.masks), \\\n",
    "            f'데이터셋 점검이 필요합니다.(inconsistency between images({len(self.images)}) ' \\\n",
    "            f'and masks({len(self.masks)}))'\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.images[index], cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks[index], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        assert self.images[index].split('/')[-1].split('.')[0] == self.masks[index].split('/')[-1].split('.')[0], '데이터셋에 이상이 있습니다.'\n",
    "\n",
    "        image, mask = cv2.resize(image, dsize=[512, 512], interpolation=cv2.INTER_NEAREST), \\\n",
    "        cv2.resize(mask,  dsize=[512, 512], interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        image, mask = self.ToTensor(image), torch.Tensor(mask).type(torch.int)\n",
    "        image = self.Normalize(image)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "class SegmentationDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 num_workers=NUM_WORKERS):\n",
    "        super(SegmentationDataModule, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = SegmentationDataset(directory=f'{DATA_DIR}/train',\n",
    "                                      transform_mode='train')\n",
    "        return DataLoader(dataset=dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = SegmentationDataset(directory=f'{DATA_DIR}/validation',\n",
    "                                      transform_mode='val')\n",
    "        return DataLoader(dataset=dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1548087c-b616-4a9d-a58c-52f362d269b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be419c49-769c-4f23-acac-ec93c18213fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../../datasets/deepfashion2/validation'\n",
    "dataset = SegmentationDataset(directory=directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af6ecc7d-0b61-46c2-a021-166ad170e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "879be774-7ccf-4056-a7e8-4d77903b6d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6705f985-acaf-4d15-b114-26de2f556510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  5, 13], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0de9a7-960b-4c11-92ae-8d27c3547a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
